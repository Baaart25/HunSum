# model
mt5:
  model_checkpoint: google/mt5-small
  max_input_length: 784
  max_output_length: 128
bert2bert:
  tokenizer: SZTAKI-HLT/hubert-base-cc

#data
train_dir: /train
valid_dir: /valid
test_dir: /test

# preprocess
do_preprocess: True
preprocessed_dataset_path: None

# training
do_train: True
output_dir: dir
resume_from_checkpoint: False

learning_rate: 0.1
weight_decay: 0.01
save_total_limit: 3
num_train_epochs: 1
save_checkpoint_steps: 5000
valid_steps: 5000
batch_size: 16
warmup_steps: 3000

# predict
do_predict: True
test_data_file: /dir/file
num_beans: 5
length_penalty: 2
no_repeat_ngram_size: 2
temperature: 0.8
top_k: 0.4

