# model
mt5:
  model_checkpoint: google/mt5-small
  max_input_length: 784
  max_output_length: 128
bert2bert:
  tokenizer: SZTAKI-HLT/hubert-base-cc
  load_model: False
  model_path: /dir

#data
train_dir: /train
valid_dir: /valid
test_dir: /test

# preprocess
do_preprocess: True
save_tokenized_data: True
preprocessed_dataset_path: None

# training
output_dir: dir
resume_from_checkpoint: False

learning_rate: 0.1
weight_decay: 0.01
save_total_limit: 3
num_train_epochs: 1
valid_steps: 5000 # same as save_checkpoint_steps
batch_size: 16
warmup_steps: 3000
fp16: False

# predict
prediction_file: None
max_predict_length: 128
num_beams: 5
length_penalty: 2
no_repeat_ngram_size: 2

