# model
mt5:
  model_checkpoint: google/mt5-small
  max_input_length: 784
  max_output_length: 128
bert2bert:
  tokenizer: SZTAKI-HLT/hubert-base-cc
  load_model: False
  model_path: /dir

#data
train_dir: /train
valid_dir: /valid
test_dir: /test
generate_dir: /test

# preprocess
do_preprocess: True
save_tokenized_data: True
preprocessed_dataset_path: None

# training
output_dir: dir
resume_from_checkpoint: False

learning_rate: 0.1
weight_decay: 0.01
save_total_limit: 3
num_train_epochs: 1
valid_steps: 5000 # same as save_checkpoint_steps
batch_size: 16
warmup_steps: 3000
fp16: False
patience: 6
compute_training_metrics: False
# used if compute_training_metrics is True
metric_for_best_model: eval_loss # bert_score_recall

# predict
prediction_file: None
max_predict_length: 128
num_beams: 5
length_penalty: 2
no_repeat_ngram_size: 2
encoder_no_repeat_ngram_size: 3
generate_early_stopping: True

